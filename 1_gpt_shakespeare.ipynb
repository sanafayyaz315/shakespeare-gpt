{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Character-Level GPT from Scratch – Training Shakespeare with Transformers\n",
        "### Goal:\n",
        "Train a small autoregressive Transformer (decoder only like GPT) to generate Shakespeare-like text at the character level.\n",
        "\n",
        "### Why:\n",
        "    - Learn the core components behind GPT models:\n",
        "\n",
        "        **Self-attention**: How the model focuses on relevant past context.\n",
        "\n",
        "        **Positional encoding**: How sequence order is encoded without recurrence.\n",
        "\n",
        "        **Autoregressive language modeling**: Predicting the next character one at a time.\n",
        "\n",
        "    - Implement a decoder-only Transformer architecture from scratch, mimicking the GPT family of models.\n",
        "\n",
        "    - Understand how attention works in practice and how it powers generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention\n",
        "The purpose of attention is to reconstruct the embedding (representation ) of a word in a vector space such that the word's vector lies closer to the words that are contexually relavant in a given sequence\n",
        "\n",
        "#### What Attention *Really* Does:\n",
        "\n",
        "> It reconstructs or refines a token’s vector (embedding) so that it moves closer in vector space to other contextually relevant words, based on the current sentence.\n",
        "> \n",
        "\n",
        "---\n",
        "\n",
        "#### Before vs After Attention\n",
        "\n",
        "- **Before attention**:\n",
        "    \n",
        "    The embedding of a word like `\"bank\"` is static — the same for `\"river bank\"` and `\"bank loan\"`.\n",
        "    \n",
        "- **After attention**:\n",
        "    \n",
        "    The word `\"bank\"` has a new vector based on **which words are nearby**.\n",
        "    \n",
        "    It shifts closer to:\n",
        "    \n",
        "    - `\"river\"`, `\"shore\"` in `\"the bank of the river\"`\n",
        "    - `\"loan\"`, `\"account\"` in `\"she went to the bank\"`\n",
        "\n",
        "---\n",
        "\n",
        "#### Why This Matters\n",
        "\n",
        "This is **how transformers handle meaning**:\n",
        "\n",
        "- They **don’t just memorize words**\n",
        "- They **compute new, dynamic meanings** for words in **every new context**\n",
        "\n",
        "---\n",
        "\n",
        "#### Intuition\n",
        "\n",
        "Imagine a 3D space:\n",
        "\n",
        "- The word `\"bank\"` is like a drone hovering in the middle.\n",
        "- Based on nearby words, **attention pulls it** toward `\"finance\"` or `\"geography\"` clusters.\n",
        "- That final position is the **contextualized meaning**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Transformer\n",
        "\n",
        "Transformer architecture was introduced in [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) as:\n",
        "\n",
        "![Transformer architecture as explained in the paper](Annotated-Transformers-Architecture.webp)\n",
        "\n",
        "**This notebook implements ONLY the DECODER block, with SELF_ATTENTION instead of CROSS-ATTENTION. This is consistent with the GPT2 architecture.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "uXoA_DRXttpp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wejugbFst4LX",
        "outputId": "cb1734f3-8285-43aa-8fd5-0363fa7e5422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-19 11:34:21--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-05-19 11:34:22 (108 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the Shakespeare  dataset -- optional\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read the dataset\n",
        "with open(\"input.txt\", 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4 # broght down from 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OCCkmtTwt5n9"
      },
      "outputs": [],
      "source": [
        "# create vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create a character level tokenizer\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "tokenizer = lambda s: [stoi[c] for c in s]\n",
        "detokenizer = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# tokenize the input\n",
        "data = torch.tensor(tokenizer(text)) # this will be the input to transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lT7DB3kuAnR",
        "outputId": "06a2f67e-1b8a-4b3b-c40f-3c474daec21d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1003854 111540\n"
          ]
        }
      ],
      "source": [
        "# split the data into train/dev splits\n",
        "# 90% data is train data, rest is val data\n",
        "n = int((0.9) * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(len(train_data), len(val_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ogtxjYQauDTl"
      },
      "outputs": [],
      "source": [
        "# create a dataloader\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # get 4 random indices\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) # shape will be (B,T)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # shape will be(B ,T)\n",
        "    x, y = x.to(device), y.to(device) # move the data to device (this is important in case device == cuda. we need to move the data to the gpu after loading)\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yDZMy13OR5K_"
      },
      "outputs": [],
      "source": [
        "class LayerNorm1d:\n",
        "\n",
        "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "        self.eps = eps # a small value called epsilon. added to the std in the denominator to avoid 0 in the denominator in case std is 0\n",
        "        # parameters (trained with backprop)\n",
        "        self.gamma = torch.ones(dim) # bngain\n",
        "        self.beta = torch.zeros(dim ) # bnbias)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # calculate the forward pass for layernorm\n",
        "        xmean = x.mean(1, keepdim=True) # mean and variance over rows\n",
        "        xvar = x.var(1, keepdim=True)\n",
        "        # standardize\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "beU3sZtEV87l"
      },
      "outputs": [],
      "source": [
        "# Self-Attention Head\n",
        "class Head(nn.Module):\n",
        "  \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    # Register causal mask (lower triangular) in variable \"tril\" as non-trainable buffer\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x) # (B, T, head_size)\n",
        "    q = self.query(x) # (B, T, head_size)\n",
        "    v = self.value(x) # (B, T, head_size)\n",
        "    # compute attention wieghts\n",
        "    wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "    # create mask\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "    # attention scores\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    wei = self.dropout(wei) # dropout in the self-attention module after calculating the attention score\n",
        "    # final attention embeddings\n",
        "    out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "    return out\n",
        "\n",
        "# Multi-head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(out) # projecting back into the residual pathway\n",
        "    out = self.dropout(out) # droput after the multihead self attention layer\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GbGuMVqK5_4C"
      },
      "outputs": [],
      "source": [
        "# feedforward class of linear layers\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd), # the inner layer has a dimensionality of 4 times the embedding size as per the transformer paper\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd), # projection layer going back into the residual pathway\n",
        "        nn.Dropout(dropout)) # add dropout before passing the connection back into the rsidual pathway\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "# grouping multihead SELF attention and feedfoward in a block\n",
        "class Block(nn.Module):\n",
        "  \"\"\"Transformer block: communication(attention) followed by computation(linear layers)\"\"\"\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd) # layernorm for the multihead-attention layer\n",
        "    self.ln2 = nn.LayerNorm(n_embd) # layernorm for the feedforward layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    # \"x + \" is adding x for residual connection/skip connection\n",
        "    # in the original paper layernorm is applied after the self attention layer and the feedfwd layer\n",
        "    # It is now common to apply it before the self attention layer and the feedfwd layer - so the input to the two sa and ffwd layers will be layernormed input\n",
        "    # this is called the \"pre-norm formulation\" and we will be implenting that\n",
        "\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uQdQ5YjEuFV2"
      },
      "outputs": [],
      "source": [
        "# Bigram Language Model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self): # removing vocab_size from the constructor since it is already defined as a global variable\n",
        "    super().__init__()\n",
        "    # nn.embedding creates a weight matrix and extracts the row corrsponding to every value in the input x matrix\n",
        "    # expanding the model to add more layers after the embedding layer\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # create an embedding lookup table for vocab_size tokens with each token represented by an n_embd size\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd) # position embedding layer - creates a position embedding for every token position (T num of tokens)\n",
        "    # self.sa_head = Head(head_sizea=n_embd) # self-attention layer with head_size same as embedding size\n",
        "    # self.sa_heads = MultiHeadAttention(4, n_embd // 4) # create 4 heads of size  so the total number of channels after concat of heads will be 32 (n_embd)\n",
        "    # self.ffwd = FeedForward(n_embd) # feedforward layer\n",
        "\n",
        "    # replacing multihead attention and ffwd net with multiple transformer blocks in a sequence\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd) # final layer norm just before the lm_head (output layer\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size) # final layer of the model called language model head\n",
        "\n",
        "  def forward(self, idx, targets=None): # idx is the input token\n",
        "    B, T = idx.shape\n",
        "    token_emb = self.token_embedding_table(idx) # (B, T, C) This will give us the embedding for every token, hence adding the channel dimension\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C) to be added to token_emd using broadcasting\n",
        "    x = token_emb + pos_emb # (B, T, C) # concat the token and pos embeddings. x holds the token identities along with their position info. Not helpful for a bigram model\n",
        "    # x = self.sa_head(x) # apply one head of self-attention (B, T, head_size)\n",
        "    # x = self.sa_heads(x) # apply multiple heads of self-attention in parallel (B, T, C)\n",
        "    # x = self.ffwd(x) # apply a feedforward layer (B, T, C)\n",
        "\n",
        "    # replacing multihead attention and ffwd net with multiple transformer blocks in a sequence\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x) # (logits) final output (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # calculate the loss\n",
        "      # Pytorch expects (B, C, T) instead of (B, T, C). Instead of reshaping to (B, C, T), we make our logits 2D\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C) # (32, 65)\n",
        "      targets = targets.view(B*T) # (32,)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to get just the last block_size number of tokens else adding pos_emb to token_embd will cause errors since we have pos_embd only for 8 tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get predictions\n",
        "      logits, loss = self(idx_cond) # this will call forward\n",
        "      # focus only on the last time step to extract the logits for the last token\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B,1) num_samples = 1 because we only sample one token at a time\n",
        "      # append sampled example to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "    return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "SKJSVc5HuHaC"
      },
      "outputs": [],
      "source": [
        "# simply printing the loss after every iteration is noisy because it the loss on a single batch\n",
        "# a better idea to view the average of loss instead\n",
        "# we average the los for train and test splits after every eval_iters number of batches\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # sets the model to eval mode. This is because some layers like dropout, batchnorm etc have different behaviours during train and test/eval time\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # sets the model to train mode\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "vQ_Dp8YJuJN-"
      },
      "outputs": [],
      "source": [
        "# initialization\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device) # move the model params like weights of the embedding table to device\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc0OwQuDaUNA",
        "outputId": "95f0db7d-5847-4dd3-a513-40ab0c9e9154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 4.5300, val loss 4.5341\n",
            "step 500: train loss 2.0929, val loss 2.1512\n",
            "step 1000: train loss 1.6635, val loss 1.8199\n",
            "step 1500: train loss 1.4902, val loss 1.6779\n",
            "step 2000: train loss 1.3910, val loss 1.6009\n",
            "step 2500: train loss 1.3178, val loss 1.5515\n",
            "step 3000: train loss 1.2660, val loss 1.5284\n",
            "step 3500: train loss 1.2232, val loss 1.5089\n",
            "step 4000: train loss 1.1824, val loss 1.4962\n",
            "step 4500: train loss 1.1468, val loss 1.4965\n",
            "1.1747770309448242\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "for iter in range(max_iters):\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # get a batch of size 32, 8 for every iteration\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate loss\n",
        "  logits, loss = m(xb, yb) # forward pass\n",
        "  optimizer.zero_grad(set_to_none=True) # set grad to 0 before backward pass\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCjnUjSNgjH9",
        "outputId": "c9bac052-bee9-4f92-fec2-046eed9fb4f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10788929"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "total_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sampling/Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg7ksYgch0pw",
        "outputId": "ee67fc81-b582-4f79-b471-571b3496b4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "And treason to there anger at the daunts\n",
            "Hot forbids to give York not lie!\n",
            "\n",
            "GLOUCESTER:\n",
            "Say night, my noble scools,\n",
            "By some eartaff'd and kill'd my sovereign\n",
            "Hard prison Chamilia: so fare your peers worth\n",
            "That you, should not see do bring them.\n",
            "\n",
            "CAPULET:\n",
            "What say we must better on that steel years?\n",
            "\n",
            "LADY CAPULET:\n",
            "There arise they sabel, though you prove!\n",
            "\n",
            "Nurse:\n",
            "Faretch, because sir, nor further lady heart.\n",
            "\n",
            "JULIET:\n",
            "What, or else dowry honger will not go and us:\n",
            "Ask thou, tutost very distrent me! I am cull'd\n",
            "Inform thy holy and Saint Cliffold spile cowards\n",
            "Ermon begins, presently, whose thrices are creeks no\n",
            "The enviom'd passable, and the kindwells: I do consul.\n",
            "\n",
            "Lords:\n",
            "Being person\n",
            "Beforedy, some that what is sue in thy sin,\n",
            "How they dare choose. Fivility, you shall not an\n",
            "Finisher sainted yours: so thy be bittern is thee\n",
            "valoued as I have made my is all;\n",
            "Think all this lets in a nay, but obey is.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "But they not not sue bear me.\n",
            "\n",
            "FRILANCE:\n",
            "Provost:\n",
            "And this daughts and mine here! where\n",
            "JULIET:\n",
            "Ay, I am sure near so nop in peace or grace.\n",
            "What would you have less,\n",
            "Let purchase for you, and not be honded encommended:\n",
            "For me, some powers at enter the noble senses,\n",
            "And so, trove as you, good uprison;\n",
            "And in the court'sympating heart of the time;\n",
            "A drunk hell, for mine.\n",
            "\n",
            "FRUMIO:\n",
            "Pray, good sheep--O, this in fool the quiet.\n",
            "\n",
            "Shepherd:\n",
            "I that like infortion true and some pactly of you banish,\n",
            "And mothers simplain how the counsel, have you but loss\n",
            "Reckows in chept depart suition my injust,\n",
            "Con breaks that may not so brings wandered.\n",
            "The dretime of thise of the lance:\n",
            "If ever thou heart inforce their advantage deceive\n",
            "To thee who shuns mose advantainted daours\n",
            "Shall even yourself untLuck and living dreads;\n",
            "The thinish would enough is palace our deep!\n",
            "Where we must glad that falter for Exetest,\n",
            "And whether by use, which eas a liventy.\n",
            "\n",
            "TYBUS:\n",
            "My lord Watchman:\n",
            "But lead the pries power a foot.\n",
            "\n",
            "Clown:\n",
            "Ay, a lord, soby!\n",
            "\n",
            "MAMILLIUS:\n",
            "None, lord,\n",
            "A mate, Conspirat\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # context also needs to be generated on the device\n",
        "print(detokenizer(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results with simpler architectures - Fewer layers and less regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq6LIrF5r1mr"
      },
      "source": [
        "#### Output after training with a single attention head (no dropout. layernorm)\n",
        "\n",
        "Thr haclle y rosthe yocou rers hout ontheed yeth on toet a ianghth to'Mi'su JZH:  \n",
        "Tnof ise cie.  \n",
        "Coof, who yof no norcay 'seat tend maibe om sheace, Iny a detsilsshe aspolen boto b.  \n",
        "\n",
        "ABndst hopor yis gealangou oit dace hag yoou  \n",
        "O'd nfurie lefonouirn bo hes syeten: nhey hod nus ousour,  \n",
        "The wohe thar Andesonfst, tich'de; wal ingat I tarru, multuth chot sat ghtou s waperllon milth hor yeoush oow thinee tou thod ghou thare  \n",
        "Ocouve thoof s,  \n",
        "ha, mouka omen.  \n",
        "Whavin yancind al.  \n",
        "\n",
        "LONNTIUS:  \n",
        "I hour pu Sowpte!  \n",
        "\n",
        "Yon Genfor, od tharint  \n",
        "Nm do hingan.  \n",
        "fnd  shon:  \n",
        "Cor wooova  \n",
        "Gpou tour tar.  \n",
        "Yu.  \n",
        "'TEW:  \n",
        "\n",
        "Ho' Rerhe Ron fllo; heee yound u hon; oarnd wirxkimnok kde o mod-d wouvefd bee Ro yodet omer foagint pe, mererom thake has ha wheak dod buraliave' prou thad skoby s keve bst sord I o ppay in, wined hinnt thakized ivesthey myre o gtacongir:  \n",
        "Cancth chin. '  \n",
        "\n",
        "Wy haved d turok in,  \n",
        "Na,  \n",
        "HLLI: andy stholou's.  \n",
        "Te he they o a  \n",
        "Hik,  \n",
        "II at hat  \n",
        "In oarringh't tinth soun a o thof the te itr onert crectrrro hy haninget heth\n",
        "\n",
        "\n",
        "**Observation**\n",
        "\n",
        "The output has started to learn the format of the input data along with some words like they, the, who. But it's still not good. Though there is improvement as compared to a simple no-attention bigram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS9BfyeR923U"
      },
      "source": [
        "### Output after multihead attention and feed forward layer(single linear layer + relu)\n",
        "\n",
        "therer penancena foughth mase hopearair doight!  \n",
        "\n",
        "ELROL: have a I moully thealsey?  \n",
        "\n",
        "Fangried seand?  \n",
        "\n",
        "Whatk you, oshoure ablaw, that the detting'dlinatte by no odent puk in Yin   \n",
        "\n",
        "desin muned thatrgy wore,  \n",
        "\n",
        "And  \n",
        "\n",
        "WerlZEN elings.  \n",
        "\n",
        "Fear frit Ble am stroth the com. andels.  \n",
        "\n",
        "No o tvorer givas; sot, Maperd'so dead. whwasl  \n",
        "myt libtheold a a ging bothic asou rkics'dcevalnst,  \n",
        "lo womn lim ehon Iould I t'sdalie a nda yamag,  \n",
        "lhat no inter.  \n",
        "achthidung. a ale ivouen geld tathill  \n",
        "Oper't: tor'd pilla  \n",
        "rsgom weorrsh: thit, I bras!  \n",
        "las Ella masinglenger; srroeap;  \n",
        "wehehe' Ea broThth ke quirty, well not child-lasichtiote a in grown meatn,  \n",
        "Tathin sthell'chel:-atheh oulde: mun creoe'st to;  \n",
        "Tawillllyt, tou be hiso Reear devy  \n",
        "rat:er athimrrens  \n",
        "Tatl loond the hem erther heyo a snot to tveacel?  \n",
        "maarn age I For OSfulffuur him broy ouerrsitel.  \n",
        "\n",
        "luen sdo So, hicheired doun, well Buucdt ille, yomprp,  \n",
        "I bioa, lend Cel you an, To they werlee willl myor;  \n",
        "W uilliore'd tran, gif isgllamtik cound loue your  \n",
        "'shou you pnone him pal  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKdbQd-o-Rh-"
      },
      "source": [
        "### Output after introducing Blocks of attention and feedforward layers\n",
        "\n",
        "The out after adding multiple blocks of attention and ffwd is still not good because the model is becoming deep and deep models have opttimization issues. SO to cater for that, we add regularization techniques as discussed in the Attention is ALL you need paper. We will be adding skip-connections (residual connections) and layer norm.\n",
        "\n",
        "\n",
        "---\n",
        "output:\n",
        "\n",
        "et ye'l kuce are sat lais hel me is  \n",
        "End sill bees by, birou my fy soist sis.  \n",
        "\n",
        "WAy Merstree my lur cagaioror,  \n",
        "The, din ling: fre craven aris, I ithio oour weve  \n",
        "Uove hiralle wold handy thate agf pome  \n",
        "Corcepe meny beatel pof thith, I gim unknt:  \n",
        "tougia po lon co ncoornoliny tol my yitht saksim lie;  \n",
        "And lind Bat od bith pof tharuar,  \n",
        "Pe teethone pucaver agier thin-stie ite,  \n",
        "Agries wa bakt man bit aye lirou torgrstonces:  \n",
        "Fhes. wilk, is fould itrcksss thing.  \n",
        "\n",
        "LALIONET:  \n",
        "\n",
        "Lird arthe fot, od; thy thow paynd, ar tho gitited  \n",
        "I wice Mrerams, my me oore, higath spard apored myough?  \n",
        "Noust olot'ly, ande oun; trountses band hesptaty.  \n",
        "Ine my sad nor, ot theco odte, tood thegh hemous.  \n",
        "Found theanay mound hirwe the.  \n",
        "Llorptse:  \n",
        "For awed orabo's t yeabjustee.  \n",
        "\n",
        "GLUK:  \n",
        "Mard norgangiord, ay waly we, capit: san mitst rore,  \n",
        "A, our pyor ooss  \n",
        "Toom bree fyoof hase the gris of arcon.  \n",
        "Sote mabller, hard ilaminud forwhe angar sostur,  \n",
        "Ond my tof toccelld has, he leay'd torge theabll exepry her to smord!  \n",
        "Heane--  \n",
        "I saver: I  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXddUbk4LBHe"
      },
      "source": [
        "## Output after applying residual connections:\n",
        "Train loss: 1.6277\n",
        "Val loss: 1.8527\n",
        "\n",
        "Observations:\n",
        "  - We start seeing some overfitting as the training loss keeps decreasing whhile val loss mostly pleatues out\n",
        "  - The output starts looking like English with some proper English vocab. The structure of the output has also starting taking shape of a dialogue.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "His your for for  \n",
        "do waitin the to in gown conspy up!  \n",
        "You priling the quonst of thou no feectrumman, die  \n",
        "Wherel: I will and I not, bid besich's!  \n",
        "And left songue done, Bence, as bethands! bodes'd.  \n",
        "\n",
        "ANGELO:  \n",
        "Had my name sir, nafor live that with Deys  \n",
        "Your of our belse weell well is dorisor  \n",
        "Is his the book for the wordloges.  \n",
        "\n",
        "ISABELA:  \n",
        "My nought. For your and swilk-  \n",
        "Beten on maser, Moculdite: if a fount-incle's.  \n",
        "Loveing as perforthe by not hound and,  \n",
        "Wife to in till friends: chaurel was your my vicdar:  \n",
        "Why, the and hasse bear,-  \n",
        "I, amdve forforther'd that hout tong shemernar's thems?  \n",
        "\n",
        "BAPTOFOZEH:  \n",
        "Conriffer:  \n",
        "Evench me, and scommand thee froud to the inn dids,  \n",
        "What samboys, that me me; forbastent togan unclectohe it.  \n",
        "\n",
        "CLAUDIO:  \n",
        "My crousofur be our here and my doth.  \n",
        "Way, good him morn her as't dismand than:  \n",
        "Vour I watland by that I not we lice in  \n",
        "And sole ontok welcingham: your refore.  \n",
        "\n",
        "GEONTHORE:  \n",
        "I am ten Edward's or mades to luge and back;  \n",
        "Vor shall if the eyem; for bance the an such nuriend's,  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME56q0thaCEl"
      },
      "source": [
        "## Output after adding LayerNorm\n",
        "Observation:\n",
        " - train loss 1.6561\n",
        " - val loss 1.8386\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Output:\n",
        "\n",
        "the cressigne-ccrant, in an other bast,  \n",
        "To bust of strung inters and beens: my amores.  \n",
        "\n",
        "LORTHUMBY:  \n",
        "Queace you me! Call citief  \n",
        "Whatch of Lord Claudo to with o'er theth toar.  \n",
        "\n",
        "CLIFFORD:  \n",
        "Why, adabist and thou.  \n",
        "\n",
        "GLOUCESTER:  \n",
        "Neved out here; boous armoon arm sonda?  \n",
        "\n",
        "WARWARD IV:  \n",
        "For HENE OF YORK:  \n",
        "Here tour Alster, my purcimph her,  \n",
        "That hast with that ear?  \n",
        "\n",
        "DUCHESSTER:  \n",
        "Struh, there my prove Boldy stretly is appin's  \n",
        "To placke say ausir is out they hurph theeford?  \n",
        "\n",
        "BRUTUST:  \n",
        "Rull dyred; if your loks oneselve sent-very.  \n",
        "\n",
        "ANEMERS:  \n",
        "This Lord, could lady learntly of with whom dow  \n",
        "Hisbled.  \n",
        "Sech, and lookes hour kasfter and the fromford  \n",
        "With Romeond, thou it eare still a go they hone:  \n",
        "Do ear-boun as as this sweears you hourstags hi  \n",
        "beh brelinge, their abouse the's ourn of there  \n",
        "'Twou'ers his so am dopplens, well.  \n",
        "\n",
        "Julstues ake how quallip't faire lasticece:  \n",
        "Beck, firberied this ornemy thrat'st hea, he vill  \n",
        "For ovide sicke them's let thee, you e's and let  \n",
        "fet thy gent the alme fort, mile an- heall pusbl  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgZSvFSwtLbw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z82TemvDxYE5"
      },
      "source": [
        "# How the Character-Level Transformer Works\n",
        "\n",
        "## Context Processing\n",
        "- The model maintains a sliding context window of **256 characters**\n",
        "- For each prediction step, it analyzes exactly **256 consecutive characters**\n",
        "- The context window contains positions `[0]` to `[255]` (zero-indexed)\n",
        "\n",
        "## Attention Mechanism\n",
        "- Self-attention computes relationships between all characters in the current window\n",
        "- Each character can only attend to:\n",
        "  - Itself\n",
        "  - Previous characters in the window (causal masking prevents looking ahead)\n",
        "- The 256th character (position `[255]`) receives contextual information from:\n",
        "  - Its own embedding\n",
        "  - All previous characters (`[0]` to `[254]`)\n",
        "\n",
        "## Prediction Process\n",
        "- The model processes all 256 characters in parallel during training\n",
        "- The final character's output embedding (position `[255]`) is used to predict:\n",
        "  - The 257th character (next token in sequence)\n",
        "- During generation:\n",
        "  - The predicted character is appended to the sequence\n",
        "  - The context window slides forward by 1 position\n",
        "  - The process repeats for the next character\n",
        "\n",
        "## Key Properties\n",
        "- **Training**: Processes full 256-character sequences simultaneously (with masking)\n",
        "- **Inference**: Generates text auto-regressively (one character at a time)\n",
        "- **Position Awareness**: Learned positional embeddings track each character's absolute position"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU9jMEZOK5hW"
      },
      "source": [
        "## Output after deepening the model and adding dropout\n",
        "Observations:\n",
        "- train loss 1.1468\n",
        "- val loss 1.4965\n",
        "---\n",
        "\n",
        "Output:\n",
        "\n",
        "\n",
        "And treason to there anger at the daunts\n",
        "Hot forbids to give York not lie!\n",
        "\n",
        "GLOUCESTER:\n",
        "Say night, my noble scools,\n",
        "By some eartaff'd and kill'd my sovereign\n",
        "Hard prison Chamilia: so fare your peers worth\n",
        "That you, should not see do bring them.\n",
        "\n",
        "CAPULET:\n",
        "What say we must better on that steel years?\n",
        "\n",
        "LADY CAPULET:\n",
        "There arise they sabel, though you prove!\n",
        "\n",
        "Nurse:\n",
        "Faretch, because sir, nor further lady heart.\n",
        "\n",
        "JULIET:\n",
        "What, or else dowry honger will not go and us:\n",
        "Ask thou, tutost very distrent me! I am cull'd\n",
        "Inform thy holy and Saint Cliffold spile cowards\n",
        "Ermon begins, presently, whose thrices are creeks no\n",
        "The enviom'd passable, and the kindwells: I do consul.\n",
        "\n",
        "Lords:\n",
        "Being person\n",
        "Beforedy, some that what is sue in thy sin,\n",
        "How they dare choose. Fivility, you shall not an\n",
        "Finisher sainted yours: so thy be bittern is thee\n",
        "valoued as I have made my is all;\n",
        "Think all this lets in a nay, but obey is.\n",
        "\n",
        "FRIAR LAURENCE:\n",
        "But they not not sue bear me.\n",
        "\n",
        "FRILANCE:\n",
        "Provost:\n",
        "And this daughts and mine here! where\n",
        "JULIET:\n",
        "Ay, I am sure near so nop in peace or grace.\n",
        "What would you have less,\n",
        "Let purchase for you, and not be honded encommended:\n",
        "For me, some powers at enter the noble senses,\n",
        "And so, trove as you, good uprison;\n",
        "And in the court'sympating heart of the time;\n",
        "A drunk hell, for mine.\n",
        "\n",
        "FRUMIO:\n",
        "Pray, good sheep--O, this in fool the quiet.\n",
        "\n",
        "Shepherd:\n",
        "I that like infortion true and some pactly of you banish,\n",
        "And mothers simplain how the counsel, have you but loss\n",
        "Reckows in chept depart suition my injust,\n",
        "Con breaks that may not so brings wandered.\n",
        "The dretime of thise of the lance:\n",
        "If ever thou heart inforce their advantage deceive\n",
        "To thee who shuns mose advantainted daours\n",
        "Shall even yourself untLuck and living dreads;\n",
        "The thinish would enough is palace our deep!\n",
        "Where we must glad that falter for Exetest,\n",
        "And whether by use, which eas a liventy.\n",
        "\n",
        "TYBUS:\n",
        "My lord Watchman:\n",
        "But lead the pries power a foot.\n",
        "\n",
        "Clown:\n",
        "Ay, a lord, soby!\n",
        "\n",
        "MAMILLIUS:\n",
        "None, lord,\n",
        "A mate, Conspirat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations\n",
        "1. Is it like Shakespeare?\n",
        "\n",
        "✅ Resemblances:\n",
        "\n",
        "Stylistic Imitation: Your model is capturing the surface-level style of Shakespeare fairly well:\n",
        "\n",
        "Capitalized character names (JULIET:, TYBUS:, FRIAR LAURENCE:).\n",
        "\n",
        "Dialogue structure with each line representing speech.\n",
        "\n",
        "Old English-sounding phrases (thou, nay, hence, etc.).\n",
        "\n",
        "Poetic rhythm, occasional use of iambic-style meter (although inconsistent).\n",
        "\n",
        "Thematic Hallucinations: Words like \"lord\", \"noble\", \"sheep\", \"watchman\", \"court\", and \"prison\" are thematic to Shakespearean drama, especially the tragedies.\n",
        "\n",
        "❌ Not Quite There Yet:\n",
        "\n",
        "Nonsense Words: Many words are completely made-up or broken:\n",
        "(fivility, tutost, infortion, frilance, advantainted, untLuck).\n",
        "\n",
        "Poor Grammar: Syntax is frequently mangled:\n",
        "\n",
        "“What, or else dowry honger will not go and us”\n",
        "\n",
        "“That you, should not see do bring them.”\n",
        "\n",
        "Missing Coherence: There's little to no narrative structure or dialogue logic. Characters respond incoherently or without relevance to one another.\n",
        "\n",
        "So yes — it imitates Shakespeare’s form impressively well for such a small model, but lacks Shakespeare’s substance (meaning, coherence, character logic).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXL7dPDGytPo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
